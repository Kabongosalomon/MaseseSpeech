{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need some install, uncomment the code bellow \n",
    "\n",
    "!pip install torch==1.4\n",
    "!pip install torchvision\n",
    "!pip install ipdb\n",
    "\n",
    "!pip install torchaudio\n",
    "!pip install PyDrive\n",
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import ipdb\n",
    "\n",
    "# from torchaudio.datasets import YESNO, LIBRISPEECH\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pdb, traceback, sys\n",
    "\n",
    "\n",
    "from torchaudio.datasets.utils import (\n",
    "  download_url,\n",
    "  extract_archive,\n",
    "  walk_files,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaseseSpeech 2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train link Drive\n",
    "# https://drive.google.com/file/d/1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR/view?usp=sharing\n",
    "    \n",
    "# wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR\" -O train-clean.tar.xz && rm -rf /tmp/cookies.txt\n",
    "    \n",
    "# # Valid link Drive\n",
    "# https://drive.google.com/file/d/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP/view?usp=sharing\n",
    "# https://drive.google.com/file/d/1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP/view?usp=sharing\n",
    "    \n",
    "# wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP\" -O dev-clean.tar.xz && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget  https://github.com/Kabongosalomon/MaseseSpeech/raw/master/voalingala.tar.xz -O voalingala.tar.xz\n",
    "\n",
    "# extract_archive(\"voalingala.tar.xz\")\n",
    "# !rm -r \"voalingala.tar.xz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir MaseseSpeech\n",
    "!mkdir MaseseSpeech\n",
    "\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1CfqHBiOEVnJiuwZoqBJMlGO-N97JD3sR\" -O MaseseSpeech/train-clean.tar.xz && rm -rf /tmp/cookies.txt\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1Y1CiB7TbrGdghVokwMTBQA1fRza2NZUP\" -O MaseseSpeech/dev-clean.tar.xz && rm -rf /tmp/cookies.txt\n",
    "\n",
    "extract_archive(\"MaseseSpeech/train-clean.tar.xz\")\n",
    "extract_archive(\"MaseseSpeech/dev-clean.tar.xz\")\n",
    "\n",
    "!rm -r \"MaseseSpeech/train-clean.tar.xz\"\n",
    "!rm -r \"MaseseSpeech/dev-clean.tar.xz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_caracters = \"Ɛ,;.ƆÁÓ-?:*É—“”!Í(10)\"    # Special caracters special to Lingala and this dataset\n",
    "tokens_list = list(\" ABCDEFGHIJKLMNOPQRSTUVWXYZ\"+special_caracters)\n",
    "tokens_set = set(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"train-clean\"\n",
    "FOLDER_IN_ARCHIVE = \"MaseseSpeech\"\n",
    "\n",
    "\n",
    "def load_masesespeech_item(fileid: str, \n",
    "                          path: str, \n",
    "                          ext_audio: str, \n",
    "                          ext_txt: str) -> Tuple[Tensor, int, str, int, int, int]:\n",
    "    \n",
    "    book_id, chapter_id, utterance_id = fileid.split(\"-\")\n",
    "    \n",
    "    file_text = book_id + \"-\" + chapter_id + ext_txt\n",
    "    file_text = os.path.join(path, book_id, chapter_id, file_text)\n",
    "    \n",
    "    fileid_audio = book_id + \"-\" + chapter_id + \"-\" + utterance_id\n",
    "    file_audio = fileid_audio + ext_audio\n",
    "    file_audio = os.path.join(path, book_id, chapter_id, file_audio)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    try :\n",
    "        # Load audio\n",
    "        waveform, sample_rate = torchaudio.load(file_audio)\n",
    "\n",
    "        # Load text\n",
    "        with open(file_text) as ft:\n",
    "            for line in ft:\n",
    "                fileid_text, utterance = line.strip().split(\" \", 1) # this takes the first space split\n",
    "                if fileid_audio == fileid_text:\n",
    "                    # stop when we found the text corresponding to \n",
    "                    # the audio ID\n",
    "                    break\n",
    "            else:\n",
    "              # Translation not found\n",
    "              raise FileNotFoundError(\"Translation not found for \" + fileid_audio)\n",
    "    except:\n",
    "        print(file_audio) # this is for debugging purpose \n",
    "        print(waveform)   # to show which file may have an issue \n",
    "        pass\n",
    "#         traceback.print_exc()\n",
    "    \n",
    "    # Use this is your acoustic model is outputting letters\n",
    "    special_caracters = \"Ɛ,;.ƆÁÓ-?:*É—“”!Í(10)\"    # Special caracters special to Lingala and this dataset\n",
    "    tokens_list = list(\" ABCDEFGHIJKLMNOPQRSTUVWXYZ\"+special_caracters)\n",
    "    tokens_set = set(tokens_list)\n",
    "    \n",
    "    transcriptions = [b for b in utterance]\n",
    "\n",
    "    t = []\n",
    "    for index in transcriptions:\n",
    "        t.append(str(tokens_list.index(index)))\n",
    "\n",
    "    targets = (\" \".join(t))\n",
    "    \n",
    "#     ipdb.set_trace()\n",
    "    with open(\"./MaseseSpeech/converted_aligned_phones.txt\", \"a+\") as text_file:              \n",
    "        # Move read cursor to the start of file.\n",
    "        text_file.seek(0)\n",
    "        # If file is not empty then append '\\n'\n",
    "        data = text_file.read(100)\n",
    "        if len(data) > 0 :\n",
    "            text_file.write(\"\\n\")\n",
    "\n",
    "        # .strip() to delect any leading and trailing whitespace\n",
    "        text_file.write(book_id+\"-\"+chapter_id+\"-\"+utterance_id+\" \"+targets)\n",
    "\n",
    "            \n",
    "    return (\n",
    "        waveform,\n",
    "        sample_rate,\n",
    "        utterance,\n",
    "        int(book_id),\n",
    "        int(chapter_id),\n",
    "        int(utterance_id),\n",
    "        )\n",
    "\n",
    "\n",
    "class MASESESPEECH_2H_MP3(Dataset):\n",
    "    \"\"\"\n",
    "    Create a Dataset for MaseseSpeech. Each item is a tuple of the form:\n",
    "    waveform, utterance, chapter_id, verse_id, utterance_id\n",
    "    \"\"\"\n",
    "    \n",
    "    _ext_txt = \".trans.txt\"\n",
    "    _ext_audio = \".wav\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 root: str,\n",
    "                 mode: str = \"MaseseSpeech/train-clean\",\n",
    "                 folder_in_archive: str = FOLDER_IN_ARCHIVE,\n",
    "                ) -> None:\n",
    "        \n",
    "        \n",
    "        self._path = mode\n",
    "        \n",
    "        walker = walk_files(\n",
    "          self._path, suffix=self._ext_audio, prefix=False, remove_suffix=True\n",
    "        )\n",
    "        self._walker = list(walker)\n",
    "    def __getitem__(self, n: int) -> Tuple[Tensor, int, str, int, int, int]:\n",
    "        fileid = self._walker[n]\n",
    "\n",
    "        return load_masesespeech_item(fileid, self._path, self._ext_audio, self._ext_txt)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self._walker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "masese_train = MASESESPEECH_2H_MP3(\".\", mode = \"MaseseSpeech/train-clean\")\n",
    "masese_dev = MASESESPEECH_2H_MP3(\".\", mode = \"MaseseSpeech/dev-clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just so you get an idea of the format \n",
    "# print(next(iter(masese_train)))\n",
    "# print(next(iter(masese_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_libri(batch):\n",
    "    #print(batch)\n",
    "    tensors = [b[0].t() for b in batch if b]\n",
    "    tensors_len = [len(t) for t in tensors]\n",
    "    tensors = torch.nn.utils.rnn.pad_sequence(tensors, batch_first=True)\n",
    "    tensors = tensors.transpose(1, -1)\n",
    "    \n",
    "#     ipdb.set_trace()\n",
    "    transcriptions = [list(b[2].replace(\"'\", \" \")) for b in batch if b]\n",
    "    targets = [torch.tensor([tokens_list.index(e) for e in t]) for t in transcriptions]\n",
    "    targets_len = [len(t) for t in targets]\n",
    "    targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "    \n",
    "    return tensors, targets, torch.tensor(tensors_len), torch.tensor(targets_len)\n",
    "\n",
    "train_set = torch.utils.data.DataLoader(masese_train, batch_size=50000, shuffle=True,\n",
    "                                        num_workers=4, collate_fn=collate_fn_libri)\n",
    "\n",
    "test_set = torch.utils.data.DataLoader(masese_dev, batch_size=50000, shuffle=True,\n",
    "                                        num_workers=4, collate_fn=collate_fn_libri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(train_set)))\n",
    "print(next(iter(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/CPC_audio.git  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/AMMI-Speech-Lig-Aikuma/Dataset/Project/CPC_audio\n"
     ]
    }
   ],
   "source": [
    "%cd CPC_audio/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!%cd /CPC_audio\n",
    "!python setup.py develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 : Building the model\n",
    "\n",
    "In this exercise, we will build and train a small CPC model using the repository CPC_audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ./CPC_audio\n",
    "from cpc.model import CPCEncoder, CPCAR\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DIM_ENCODER = 256 \n",
    "DIM_CONTEXT = 256 \n",
    "KEEP_HIDDEN_VECTOR = False\n",
    "N_LEVELS_CONTEXT = 1\n",
    "CONTEXT_RNN = \"LSTM\"\n",
    "N_PREDICTIONS = 12\n",
    "LEARNING_RATE = 2e-4\n",
    "N_NEGATIVE_SAMPLE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CPCEncoder(DIM_ENCODER)\n",
    "context = CPCAR(DIM_ENCODER,\n",
    "                DIM_CONTEXT,\n",
    "                KEEP_HIDDEN_VECTOR,\n",
    "                N_LEVELS_CONTEXT,\n",
    "                mode=\"CONTEXT_RNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo apt-get install libsndfile1-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several functions that will be necessary to load the data later\n",
    "from cpc.dataset import findAllSeqs, AudioBatchData, parseSeqLabels\n",
    "SIZE_WINDOW = 20480\n",
    "BATCH_SIZE=8\n",
    "def load_dataset(path_dataset, file_extension='.wav', phone_label_dict=None):\n",
    "    data_list, speakers = findAllSeqs(path_dataset, extension=file_extension)\n",
    "    dataset = AudioBatchData(path_dataset, SIZE_WINDOW, data_list, phone_label_dict, len(speakers))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPCModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 encoder,\n",
    "                 AR):\n",
    "\n",
    "        super(CPCModel, self).__init__()\n",
    "        self.gEncoder = encoder\n",
    "        self.gAR = AR\n",
    "\n",
    "    def forward(self, batch_data):\n",
    "        \n",
    "\n",
    "        encoder_output = self.gEncoder(batch_data)\n",
    "        #print(encoder_output.shape)\n",
    "        # The output of the encoder data does not have the good format \n",
    "        # indeed it is Batch_size x Hidden_size x temp size\n",
    "        # while the context requires Batch_size  x temp size x Hidden_size\n",
    "        # thus you need to permute\n",
    "        context_input = encoder_output.permute(0, 2, 1)\n",
    "\n",
    "        context_output = self.gAR(context_input)\n",
    "        #print(context_output.shape)\n",
    "        return context_output, encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = torchaudio.load(\n",
    "#         \"../voalingala/20200611/20200611-160000-VCD361-program_16k.mp3\")[0]\n",
    "        \"../MaseseSpeech/train-clean/020/001/020-001-013.wav\")[0]\n",
    "        \n",
    "\n",
    "audio = audio.view(1, 1, -1)\n",
    "\n",
    "cpc_model = CPCModel(encoder, context).to(device)\n",
    "context_output, encoder_output = cpc_model(audio.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.1118, 0.0728, 0.0479]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_30.pt    checkpoint_args.json    checkpoint_logs.json\n",
      "checkpoint_30.pt.1  checkpoint_args.json.1  checkpoint_logs.json.1\n"
     ]
    }
   ],
   "source": [
    "!ls checkpoint_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html # in case you're getting nvdia error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cpc.dataset import parseSeqLabels\n",
    "# from cpc.feature_loader import loadModel\n",
    "\n",
    "# checkpoint_path = 'checkpoint_data/checkpoint_30.pt'\n",
    "# cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n",
    "# cpc_model = cpc_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 : CPC loss\n",
    "\n",
    "We will define a class ```CPCCriterion``` which will hold the prediction networks $\\phi_k$ defined above and perform the classification loss $\\mathcal{L}_c$.\n",
    "\n",
    "a) In this exercise, the $\\phi_k$ will be a linear transform, ie:\n",
    "\n",
    "\\\\[ \\phi_k(c_t) = \\mathbf{A}_k c_t\\\\]\n",
    "\n",
    "Using the class [torch.nn.Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear), define the transformations $\\phi_k$ in the code below and complete the function ```get_prediction_k``` which computes $\\phi_k(c_t)$ for a given batch of vectors $c_t$.\n",
    "\n",
    "b) Using both ```get_prediction_k```  and ```sample_negatives``` defined below, write the forward function which will take as input two batches of features $c_t$ and $g_t$ and outputs the classification loss $\\mathcal{L}_c$ and the average acuracy for all predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice 2: write the CPC loss\n",
    "# a) Write the negative sampling (with some help)\n",
    "# ERRATUM: it's really hard, the sampling will be provided\n",
    "\n",
    "class CPCCriterion(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 K,\n",
    "                 dim_context,\n",
    "                 dim_encoder,\n",
    "                 n_negative):\n",
    "        \n",
    "        super(CPCCriterion, self).__init__()\n",
    "        \n",
    "        self.K_ = K\n",
    "        self.dim_context = dim_context\n",
    "        self.dim_encoder = dim_encoder\n",
    "        self.n_negative = n_negative\n",
    "\n",
    "        self.predictors = torch.nn.ModuleList()\n",
    "        \n",
    "        for k in range(self.K_):\n",
    "            # TO COMPLETE !\n",
    "            # A affine transformation in pytorch is equivalent to a nn.Linear layer\n",
    "            # To get a linear transformation you must set bias=False\n",
    "            # input dimension of the layer = dimension of the encoder\n",
    "            # output dimension of the layer = dimension of the context\n",
    "            self.predictors.append(torch.nn.Linear(dim_context, dim_encoder, bias=False))\n",
    "        \n",
    "    def get_prediction_k(self, context_data):\n",
    "        #TO COMPLETE !\n",
    "        output = [] \n",
    "        # For each time step k\n",
    "        for k in range(self.K_):\n",
    "            # We need to compute phi_k = A_k * c_t\n",
    "            phi_k = self.predictors[k](context_data)\n",
    "            output.append(phi_k)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def sample_negatives(self, encoded_data):\n",
    "        r\"\"\"\n",
    "        Sample some negative examples in the given encoded data.\n",
    "        Input:\n",
    "        - encoded_data size: B x T x H\n",
    "        Returns\n",
    "        - outputs of size B x (n_negative + 1) x (T - K_) x H\n",
    "          outputs[:, 0, :, :] contains the positive example\n",
    "          outputs[:, 1:, :, :] contains negative example sampled in the batch\n",
    "        - labels, long tensor of size B x (T - K_)\n",
    "          Since the positive example is always at coordinates 0 for all sequences \n",
    "          in the batch and all timestep in the sequence, labels is just a tensor\n",
    "          full of zeros !\n",
    "        \"\"\"\n",
    "        batch_size, time_size, dim_encoded = encoded_data.size()\n",
    "        window_size = time_size - self.K_\n",
    "        outputs = []\n",
    "\n",
    "        neg_ext = encoded_data.contiguous().view(-1, dim_encoded)\n",
    "        n_elem_sampled = self.n_negative * window_size * batch_size\n",
    "        # Draw nNegativeExt * batchSize negative samples anywhere in the batch\n",
    "        batch_idx = torch.randint(low=0, high=batch_size,\n",
    "                                  size=(n_elem_sampled, ),\n",
    "                                  device=encoded_data.device)\n",
    "\n",
    "        seq_idx = torch.randint(low=1, high=time_size,\n",
    "                                size=(n_elem_sampled, ),\n",
    "                                device=encoded_data.device)\n",
    "\n",
    "        base_idx = torch.arange(0, window_size, device=encoded_data.device)\n",
    "        base_idx = base_idx.view(1, 1, window_size)\n",
    "        base_idx = base_idx.expand(1, self.n_negative, window_size)\n",
    "        base_idx = base_idx.expand(batch_size, self.n_negative, window_size)\n",
    "        seq_idx += base_idx.contiguous().view(-1)\n",
    "        seq_idx = torch.remainder(seq_idx, time_size)\n",
    "\n",
    "        ext_idx = seq_idx + batch_idx * time_size\n",
    "        neg_ext = neg_ext[ext_idx].view(batch_size, self.n_negative,\n",
    "                                        window_size, dim_encoded)\n",
    "        label_loss = torch.zeros((batch_size, window_size),\n",
    "                                  dtype=torch.long,\n",
    "                                  device=encoded_data.device)\n",
    "\n",
    "        for k in range(1, self.K_ + 1):\n",
    "            # Positive samples\n",
    "            if k < self.K_:\n",
    "                pos_seq = encoded_data[:, k:-(self.K_-k)]\n",
    "            else:\n",
    "                pos_seq = encoded_data[:, k:]\n",
    "            pos_seq = pos_seq.view(batch_size, 1, pos_seq.size(1), dim_encoded)\n",
    "            full_seq = torch.cat((pos_seq, neg_ext), dim=1)\n",
    "            outputs.append(full_seq)\n",
    "\n",
    "        return outputs, label_loss\n",
    "    \n",
    "    def forward(self, encoded_data, context_data):\n",
    "\n",
    "        # TO COMPLETE:\n",
    "        # Perform the full cpc criterion\n",
    "        # Returns 2 values:\n",
    "        # - the average classification loss avg_loss\n",
    "        # - the average classification acuracy avg_acc\n",
    "\n",
    "        # Reminder : The permuation !\n",
    "        encoded_data = encoded_data.permute(0, 2, 1)\n",
    "\n",
    "        # First we need to sample the negative examples\n",
    "        negative_samples, labels = self.sample_negatives(encoded_data)\n",
    "\n",
    "        # Then we must compute phi_k\n",
    "        phi_k = self.get_prediction_k(context_data)\n",
    "\n",
    "        # Finally we must get the dot product between phi_k and negative_samples \n",
    "        # for each k\n",
    "\n",
    "        #The total loss is the average of all losses\n",
    "        avg_loss = 0\n",
    "\n",
    "        # Average acuracy\n",
    "        avg_acc = 0\n",
    "\n",
    "        for k in range(self.K_):\n",
    "            B, N_sampled, S_small, H = negative_samples[k].size() \n",
    "            B, S, H = phi_k[k].size()\n",
    "            \n",
    "            # As told before S = S_small + K. For segments too far in the sequence\n",
    "            # there are no positive exmples anyway, so we must shorten phi_k\n",
    "            phi = phi_k[k][:, :S_small]\n",
    "            \n",
    "            # Now the dot product\n",
    "            # You have several ways to do that, let's do the simple but non optimal \n",
    "            # one\n",
    "            # pytorch has a matrix product function https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
    "            # But it takes only 3D tensors of the same batch size !\n",
    "            # To begin negative_samples is a 4D tensor ! \n",
    "            # We want to compute the dot product for each features, of each sequence\n",
    "            # of the batch. Thus we are trying to compute a dot product for all\n",
    "            # B* N_sampled * S_small 1D vector of negative_samples[k]\n",
    "            # Or, a 1D tensor of size H is also a matrix of size 1 x H\n",
    "            # Then, we must view it as a 3D tensor of size (B* N_sampled * S_small, 1, H)\n",
    "            negative_sample_k  =  negative_samples[k].view(B* N_sampled* S_small, 1, H)\n",
    "            \n",
    "            # But now phi and negative_sample_k no longer have the same batch size !\n",
    "            # No worries, we can expand phi so that each sequence of the batch\n",
    "            # is repeated N_sampled times\n",
    "            phi = phi.view(B, 1,S_small, H).expand(B, N_sampled, S_small, H)\n",
    "            \n",
    "            # And now we can view it as a 3D tensor \n",
    "            phi  = phi.contiguous().view(B * N_sampled * S_small, H, 1)\n",
    "            \n",
    "            # We can finally get the dot product !\n",
    "            scores = torch.bmm(negative_sample_k, phi)\n",
    "            \n",
    "            # Dot_product has a size (B * N_sampled * S_small , 1, 1)\n",
    "            # Let's reorder it a bit\n",
    "            scores = scores.reshape(B, N_sampled, S_small)\n",
    "            \n",
    "            # For each elements of the sequence, and each elements sampled, it gives \n",
    "            # a floating score stating the likelihood of this element being the \n",
    "            # true one.\n",
    "            # Now the classification loss, we need to use the Cross Entropy loss\n",
    "            # https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html\n",
    "            \n",
    "            # For each time-step of each sequence of the batch \n",
    "            # we have N_sampled possible predictions. \n",
    "            # Looking at the documentation of torch.nn.CrossEntropyLoss\n",
    "            # we can see that this loss expect a tensor of size M x C where \n",
    "            # - M is the number of elements with a classification score\n",
    "            # - C is the number of possible classes\n",
    "            # There are N_sampled candidates for each predictions so\n",
    "            # C = N_sampled \n",
    "            # Each timestep of each sequence of the batch has a prediction so\n",
    "            # M = B * S_small\n",
    "            # Thus we need an input vector of size B * S_small, N_sampled\n",
    "            # To begin, we need to permute the axis\n",
    "            scores = scores.permute(0, 2, 1) # Now it has size B , S_small, N_sampled\n",
    "            \n",
    "            # Then we can cast it into a 2D tensor\n",
    "            scores = scores.reshape(B * S_small, N_sampled)\n",
    "            \n",
    "            # Same thing for the labels \n",
    "            labels = labels.reshape(B * S_small)\n",
    "            \n",
    "            # Finally we can get the classification loss\n",
    "            loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "            loss_k = loss_criterion(scores, labels)\n",
    "            avg_loss+= loss_k\n",
    "            \n",
    "            # And for the acuracy\n",
    "            # The prediction for each elements is the sample with the highest score\n",
    "            # Thus the tensors of all predictions is the tensors of the index of the \n",
    "            # maximal score for each time-step of each sequence of the batch\n",
    "            predictions = torch.argmax(scores, 1)\n",
    "            acc_k  = (labels == predictions).sum() / (B * S_small)\n",
    "            avg_acc += acc_k\n",
    "\n",
    "        # Normalization\n",
    "        avg_loss = avg_loss / self.K_\n",
    "        avg_acc = avg_acc / self.K_\n",
    "        \n",
    "        return avg_loss , avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ../MaseseSpeech/train-clean/020/001/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "audio = torchaudio.load(\n",
    "    \"../MaseseSpeech/train-clean/020/001/020-001-013.wav\")[0]\n",
    "#     \"../voalingala/20200611/20200611-160000-VCD361-program_16k.mp3\")[0]\n",
    "\n",
    "audio = audio.view(1, 1, -1)\n",
    "cpc_criterion = CPCCriterion(N_PREDICTIONS, DIM_CONTEXT, \n",
    "                             DIM_ENCODER, N_NEGATIVE_SAMPLE).to(device)\n",
    "context_output, encoder_output = cpc_model(audio.to(device))\n",
    "loss, avg = cpc_criterion(encoder_output,context_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3921, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Full training loop !\n",
    "\n",
    "You have the model, you have the criterion. All you need now are a data loader and an optimizer to run your training loop.\n",
    "\n",
    "We will use an Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = list(cpc_criterion.parameters()) + list(cpc_model.parameters())\n",
    "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 5564.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cache file at ../MaseseSpeech/train-clean/_seqs_cache.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "363it [00:00, 551642.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking length...\n",
      "Done, elapsed: 0.049 seconds\n",
      "Scanned 363 sequences in 0.05 seconds\n",
      "1 chunks computed\n",
      "Joining pool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined process, elapsed=0.839 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:00, 106.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cache file at ../MaseseSpeech/dev-clean/_seqs_cache.txt\n",
      "Checking length...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "275it [00:00, 450208.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, elapsed: 2.133 seconds\n",
      "Scanned 275 sequences in 2.13 seconds\n",
      "1 chunks computed\n",
      "Joining pool\n",
      "Joined process, elapsed=4.615 secs\n"
     ]
    }
   ],
   "source": [
    "# dataset_train = load_dataset('../voalingala/train',\n",
    "#                              file_extension='.mp3')\n",
    "# dataset_val = load_dataset('../voalingala/val',\n",
    "#                              file_extension='.mp3')\n",
    "\n",
    "dataset_train = load_dataset('../MaseseSpeech/train-clean')\n",
    "dataset_val = load_dataset('../MaseseSpeech/dev-clean')\n",
    "\n",
    "data_loader_train = dataset_train.getDataLoader(BATCH_SIZE, \"speaker\", True)\n",
    "data_loader_val = dataset_train.getDataLoader(BATCH_SIZE, \"sequence\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(data_loader,\n",
    "               cpc_model,\n",
    "               cpc_criterion,\n",
    "               optimizer):\n",
    "    \n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    n_items = 0\n",
    "    \n",
    "    for step, data in enumerate(data_loader):\n",
    "        x,y = data\n",
    "        bs = len(x)\n",
    "        optimizer.zero_grad()\n",
    "        context_output, encoder_output = cpc_model(x.to(device))\n",
    "        loss , acc = cpc_criterion(encoder_output, context_output)\n",
    "        loss.backward()\n",
    "        n_items+=bs\n",
    "        avg_loss+=loss.item()*bs\n",
    "        avg_acc +=acc.item()*bs\n",
    "    avg_loss/=n_items\n",
    "    avg_acc/=n_items\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 : Validation loop\n",
    "\n",
    "Now complete the validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(data_loader,\n",
    "                    cpc_model,\n",
    "                    cpc_criterion):\n",
    "    \n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    n_items = 0\n",
    "    \n",
    "    for step, data in enumerate(data_loader):\n",
    "        x,y = data\n",
    "        bs = len(x)\n",
    "        context_output, encoder_output = cpc_model(x.to(device))\n",
    "        loss , acc = cpc_criterion(encoder_output, context_output)\n",
    "        n_items+=bs\n",
    "        avg_loss+=loss.item()*bs\n",
    "        avg_acc+=acc.item()*bs\n",
    "        \n",
    "    avg_loss/=n_items\n",
    "    avg_acc/=n_items\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OBVUPKKs2_0U"
   },
   "source": [
    "## Exercise 5: Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(train_loader,\n",
    "        val_loader,\n",
    "        cpc_model,\n",
    "        cpc_criterion,\n",
    "        optimizer,\n",
    "        n_epochs):\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Running epoch {epoch+1} / {n_epochs}\")\n",
    "        avg_loss_train, avg_acc_train = train_step(train_loader, cpc_model, cpc_criterion, optimizer)\n",
    "        print(\"----------------------\")\n",
    "        print(f\"Training dataset\")\n",
    "        print(f\"- average loss : {avg_loss_train}\")\n",
    "        print(f\"- average acuracy : {avg_acc_train}\")\n",
    "        print(\"----------------------\")\n",
    "        with torch.no_grad():\n",
    "            cpc_model.eval()\n",
    "            cpc_criterion.eval()\n",
    "            avg_loss_val, avg_acc_val = validation_step(val_loader, cpc_model, cpc_criterion)\n",
    "            print(f\"Validation dataset\")\n",
    "            print(f\"- average loss : {avg_loss_val}\")\n",
    "            print(f\"- average acuracy : {avg_acc_val}\")\n",
    "            print(\"----------------------\")\n",
    "            print()\n",
    "            cpc_model.train()\n",
    "            cpc_criterion.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1 / 1\n",
      "----------------------\n",
      "Training dataset\n",
      "- average loss : 5.365146972868177\n",
      "- average acuracy : 0.0\n",
      "----------------------\n",
      "Validation dataset\n",
      "- average loss : 5.365160039265951\n",
      "- average acuracy : 0.0\n",
      "----------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run(data_loader_train, data_loader_val, cpc_model,cpc_criterion,optimizer,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once everything is donw, clear the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset_train\n",
    "del dataset_val\n",
    "del cpc_model\n",
    "del context\n",
    "del encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srPM5r_LB9v-"
   },
   "source": [
    "# Part 2 : Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Nb_-0IQiJk9"
   },
   "source": [
    "## Exercice 1 : Phone separability with aligned phonemes.\n",
    "\n",
    "One option to **evaluate the quality of the features trained with CPC can be to check if they can be used to recognize phonemes**. \n",
    "To do so, we can fine-tune a **pre-trained model using a limited amount of labelled speech data**.\n",
    "We are going to start with a simple evaluation setting where we have the phone labels for each timestep corresponding to a CPC feature.\n",
    "\n",
    "We will work with a model already pre-trained on English data. As far as the fine-tuning dataset is concerned, we will use a 1h subset of [librispeech-100](http://www.openslr.org/12/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "N-scDMAasXxc",
    "outputId": "3ae4d269-ccba-405e-ad08-da80e85a7e7a"
   },
   "outputs": [],
   "source": [
    "!mkdir checkpoint_data\n",
    "!wget https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_30.pt -P checkpoint_data\n",
    "!wget https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_logs.json -P checkpoint_data\n",
    "!wget https://dl.fbaipublicfiles.com/librilight/CPC_checkpoints/not_hub/2levels_6k_top_ctc/checkpoint_args.json -P checkpoint_data\n",
    "!ls checkpoint_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m020\u001b[0m/  _seqs_cache.txt\n"
     ]
    }
   ],
   "source": [
    "ls ../MaseseSpeech/dev-clean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "SSSaiYo82_oY",
    "outputId": "cbd1917b-6d69-4eaf-a564-62f52aa9ba6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint_data/checkpoint_30.pt\n",
      "Loading the state dict at checkpoint_data/checkpoint_30.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 5006.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cache file at ../MaseseSpeech/train-clean/_seqs_cache.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "363it [00:00, 352928.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking length...\n",
      "Done, elapsed: 0.056 seconds\n",
      "Scanned 363 sequences in 0.06 seconds\n",
      "1 chunks computed\n",
      "Joining pool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined process, elapsed=0.927 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:00, 562.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cache file at ../MaseseSpeech/dev-clean/_seqs_cache.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "275it [00:00, 323907.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking length...\n",
      "Done, elapsed: 0.042 seconds\n",
      "Scanned 275 sequences in 0.04 seconds\n",
      "1 chunks computed\n",
      "Joining pool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined process, elapsed=0.698 secs\n"
     ]
    }
   ],
   "source": [
    "# %cd /content/CPC_audio\n",
    "from cpc.dataset import parseSeqLabels\n",
    "from cpc.feature_loader import loadModel\n",
    "\n",
    "checkpoint_path = 'checkpoint_data/checkpoint_30.pt'\n",
    "cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n",
    "cpc_model = cpc_model.cuda()\n",
    "\n",
    "label_dict, N_PHONES = parseSeqLabels('../MaseseSpeech/converted_aligned_phones.txt')\n",
    "dataset_train = load_dataset('../MaseseSpeech/train-clean/', file_extension='.wav', phone_label_dict=label_dict)\n",
    "dataset_val = load_dataset('../MaseseSpeech/dev-clean/', file_extension='.wav', phone_label_dict=label_dict)\n",
    "\n",
    "data_loader_train = dataset_train.getDataLoader(BATCH_SIZE, \"speaker\", True)\n",
    "data_loader_val = dataset_val.getDataLoader(BATCH_SIZE, \"sequence\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkKi-qfosng2"
   },
   "source": [
    "Then we will use a simple linear classifier to recognize the phonemes from the features produced by ```cpc_model```. \n",
    "\n",
    "### a) Build the phone classifier \n",
    "\n",
    "Design a class of linear classifiers, ```PhoneClassifier``` that will take as input a batch of sequences of CPC features and output a score vector for each phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4RpAbz-0CXJJ"
   },
   "outputs": [],
   "source": [
    "class PhoneClassifier(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim : int,\n",
    "                 n_phones : int):\n",
    "        super(PhoneClassifier, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, n_phones)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zt5oa_nqtH-d"
   },
   "source": [
    "Our phone classifier will then be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRBf_83IuLv5"
   },
   "outputs": [],
   "source": [
    "phone_classifier = PhoneClassifier(HIDDEN_CONTEXT_MODEL, N_PHONES).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z_Vf5AbUhqm4"
   },
   "source": [
    "### b - What would be the correct loss criterion for this task ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhyPM-cgjrtw"
   },
   "outputs": [],
   "source": [
    "loss_criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nv4cSxbaplrz"
   },
   "source": [
    "To perform the fine-tuning, we will also need an optimization function.\n",
    "\n",
    "We will use an [Adam optimizer ](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W5CgYyAlqKxu"
   },
   "outputs": [],
   "source": [
    "parameters = list(phone_classifier.parameters()) + list(cpc_model.parameters())\n",
    "LEARNING_RATE = 2e-4\n",
    "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qQB9HS9PvAXc"
   },
   "source": [
    "You might also want to perform this training while freezing the weights of the ```cpc_model```. Indeed, if the pre-training was good enough, then ```cpc_model``` phonemes representation should be linearly separable. In this case the optimizer should be defined like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRy0gn6awGUQ"
   },
   "outputs": [],
   "source": [
    "optimizer_frozen = torch.optim.Adam(list(phone_classifier.parameters()), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cO93ngIfj4JW"
   },
   "source": [
    "### c- Now let's build a training loop. \n",
    "Complete the function ```train_one_epoch``` below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fabqj3wvLwgU"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(cpc_model, \n",
    "                    phone_classifier, \n",
    "                    loss_criterion, \n",
    "                    data_loader, \n",
    "                    optimizer):\n",
    "    \n",
    "    cpc_model.train()\n",
    "    loss_criterion.train()\n",
    "    \n",
    "    avg_loss = 0\n",
    "    avg_accuracy = 0\n",
    "    n_items = 0\n",
    "    for step, full_data in enumerate(data_loader):\n",
    "        # Each batch is represented by a Tuple of vectors:\n",
    "        # sequence of size : N x 1 x T\n",
    "        # label of size : N x T\n",
    "        # \n",
    "        # With :\n",
    "        # - N number of sequence in the batch\n",
    "        # - T size of each sequence\n",
    "        sequence, label = full_data\n",
    "\n",
    "\n",
    "\n",
    "        bs = len(sequence)\n",
    "        seq_len = label.size(1)\n",
    "        optimizer.zero_grad()\n",
    "        context_out, enc_out, _ = cpc_model(sequence.to(device),label.to(device))\n",
    "\n",
    "        scores = phone_classifier(context_out)\n",
    "\n",
    "        scores = scores.permute(0,2,1)\n",
    "        loss = loss_criterion(scores,label.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss+=loss.item()*bs\n",
    "        n_items+=bs\n",
    "        correct_labels = scores.argmax(1)\n",
    "        avg_accuracy += ((label==correct_labels.cpu()).float()).mean(1).sum().item()\n",
    "        \n",
    "    avg_loss/=n_items\n",
    "    avg_accuracy/=n_items\n",
    "    \n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "quYtjx_TxIPK"
   },
   "source": [
    "Don't forget to test it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50MwxbKhxMKp"
   },
   "outputs": [],
   "source": [
    "avg_loss, avg_accuracy = train_one_epoch(cpc_model, phone_classifier, loss_criterion, data_loader_train, optimizer_frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_o6yk8XKWnYe",
    "outputId": "19b006e6-03ac-4c18-c415-cdeb293b54d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.769851134883033, 0.049397786458333336)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EmUkuJ2bwu4Z"
   },
   "source": [
    "### d- Build the validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZJMxj6cwzd3"
   },
   "outputs": [],
   "source": [
    "def validation_step(cpc_model, \n",
    "                    phone_classifier, \n",
    "                    loss_criterion, \n",
    "                    data_loader):\n",
    "    \n",
    "    cpc_model.eval()\n",
    "    phone_classifier.eval()\n",
    "    \n",
    "    avg_loss = 0\n",
    "    avg_accuracy = 0\n",
    "    n_items = 0\n",
    "    with torch.no_grad():\n",
    "        for step, full_data in enumerate(data_loader):\n",
    "            # Each batch is represented by a Tuple of vectors:\n",
    "            # sequence of size : N x 1 x T\n",
    "            # label of size : N x T\n",
    "            # \n",
    "            # With :\n",
    "            # - N number of sequence in the batch\n",
    "            # - T size of each sequence\n",
    "            sequence, label = full_data\n",
    "            bs = len(sequence)\n",
    "            seq_len = label.size(1)\n",
    "            context_out, enc_out, _ = cpc_model(sequence.to(device),label.to(device))\n",
    "            scores = phone_classifier(context_out)\n",
    "            scores = scores.permute(0,2,1)\n",
    "            loss = loss_criterion(scores,label.to(device))\n",
    "            avg_loss+=loss.item()*bs\n",
    "            n_items+=bs\n",
    "            correct_labels = scores.argmax(1)\n",
    "            avg_accuracy += ((label==correct_labels.cpu()).float()).mean(1).sum().item()\n",
    "            \n",
    "    avg_loss/=n_items\n",
    "    avg_accuracy/=n_items\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vownVCt7xbVh"
   },
   "source": [
    "### e- Run everything\n",
    "\n",
    "Test this functiion with both ```optimizer``` and ```optimizer_frozen```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xvO_4nKUxfQx"
   },
   "outputs": [],
   "source": [
    "def run(cpc_model, \n",
    "        phone_classifier, \n",
    "        loss_criterion, \n",
    "        data_loader_train, \n",
    "        data_loader_val, \n",
    "        optimizer,\n",
    "        n_epoch):\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        \n",
    "        print(f\"Running epoch {epoch + 1} / {n_epoch}\")\n",
    "        loss_train, acc_train = train_one_epoch(cpc_model, phone_classifier, loss_criterion, data_loader_train, optimizer)\n",
    "        print(\"-------------------\")\n",
    "        print(f\"Training dataset :\")\n",
    "        print(f\"Average loss : {loss_train}. Average accuracy {acc_train}\")\n",
    "\n",
    "        print(\"-------------------\")\n",
    "        print(\"Validation dataset\")\n",
    "        loss_val, acc_val = validation_step(cpc_model, phone_classifier, loss_criterion, data_loader_val)\n",
    "        print(f\"Average loss : {loss_val}. Average accuracy {acc_val}\")\n",
    "        print(\"-------------------\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ceCEO2h2bxAn",
    "outputId": "2842125e-7f20-434d-9189-33699670f620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 3.563035892115699. Average accuracy 0.09114583333333333\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 3.4712906800783596. Average accuracy 0.09735576923076923\n",
      "-------------------\n",
      "\n",
      "Running epoch 2 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 3.397801478703817. Average accuracy 0.10579427083333333\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 3.3246844915243297. Average accuracy 0.11399489182692307\n",
      "-------------------\n",
      "\n",
      "Running epoch 3 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 3.266044795513153. Average accuracy 0.1283908420138889\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 3.210064704601581. Average accuracy 0.14107572115384615\n",
      "-------------------\n",
      "\n",
      "Running epoch 4 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 3.16418319940567. Average accuracy 0.14561631944444445\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 3.1193005305070143. Average accuracy 0.15459735576923078\n",
      "-------------------\n",
      "\n",
      "Running epoch 5 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 3.0817254847950406. Average accuracy 0.1541069878472222\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 3.048164257636437. Average accuracy 0.15771484375\n",
      "-------------------\n",
      "\n",
      "Running epoch 6 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 3.0178873936335244. Average accuracy 0.1553276909722222\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 2.9911002287497888. Average accuracy 0.15899188701923078\n",
      "-------------------\n",
      "\n",
      "Running epoch 7 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 2.966975278324551. Average accuracy 0.15831163194444445\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 2.9479496020537157. Average accuracy 0.15940504807692307\n",
      "-------------------\n",
      "\n",
      "Running epoch 8 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 2.9286783602502613. Average accuracy 0.1586642795138889\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 2.9129452521984396. Average accuracy 0.15966796875\n",
      "-------------------\n",
      "\n",
      "Running epoch 9 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 2.8985020253393383. Average accuracy 0.1583930121527778\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 2.8869830003151526. Average accuracy 0.16064453125\n",
      "-------------------\n",
      "\n",
      "Running epoch 10 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 2.873071485095554. Average accuracy 0.16151258680555555\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 2.8657545951696544. Average accuracy 0.16192157451923078\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run(cpc_model,phone_classifier,loss_criterion,data_loader_train,data_loader_val,optimizer_frozen,n_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TdfWDiFnylMT"
   },
   "source": [
    "## Exercise 2 : Phone separability without alignment (PER)\n",
    "\n",
    "**Aligned data are very practical, but un real life they are rarely available.** That's why in this excercise we will consider a **fine-tuning with non-aligned phonemes.**\n",
    "\n",
    "The model, the optimizer and the phone classifier will stay the same. However, we will replace our phone criterion with a [CTC loss](https://pytorch.org/docs/master/generated/torch.nn.CTCLoss.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9BpM_Lpzgx8"
   },
   "outputs": [],
   "source": [
    "loss_ctc = torch.nn.CTCLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQpYgTyfzsrq"
   },
   "source": [
    "Besides, we will use a siglthy different dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "9HRxoatlz3ZZ",
    "outputId": "210b7f5f-b722-42dd-97df-56cb2e614a1a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 4916.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cache file at ../MaseseSpeech/train-clean/_seqs_cache.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "12it [00:00, 5078.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 364 sequences in 2.21 seconds\n",
      "maxSizeSeq : 406560\n",
      "maxSizePhone : 199\n",
      "minSizePhone : 17\n",
      "Total size dataset 1.28108578125 hours\n",
      "Saved cache file at ../MaseseSpeech/dev-clean/_seqs_cache.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 276 sequences in 2.20 seconds\n",
      "maxSizeSeq : 356829\n",
      "maxSizePhone : 175\n",
      "minSizePhone : 57\n",
      "Total size dataset 0.9148587326388888 hours\n"
     ]
    }
   ],
   "source": [
    "# %cd /content/CPC_audio\n",
    "from cpc.eval.common_voices_eval import SingleSequenceDataset, parseSeqLabels, findAllSeqs\n",
    "path_train_data_per = '../MaseseSpeech/train-clean/'\n",
    "path_val_data_per = '../MaseseSpeech/dev-clean'\n",
    "path_phone_data_per = '../MaseseSpeech/converted_aligned_phones.txt'\n",
    "\n",
    "\n",
    "BATCH_SIZE=8\n",
    "\n",
    "phone_labels, N_PHONES = parseSeqLabels(path_phone_data_per)\n",
    "data_train_per, _ = findAllSeqs(path_train_data_per, extension='.wav')\n",
    "dataset_train_non_aligned = SingleSequenceDataset(path_train_data_per, data_train_per, phone_labels)\n",
    "data_loader_train = torch.utils.data.DataLoader(dataset_train_non_aligned, batch_size=BATCH_SIZE,\n",
    "                                                shuffle=True)\n",
    "\n",
    "data_val_per, _ = findAllSeqs(path_val_data_per, extension='.wav')\n",
    "dataset_val_non_aligned = SingleSequenceDataset(path_val_data_per, data_val_per, phone_labels)\n",
    "data_loader_val = torch.utils.data.DataLoader(dataset_val_non_aligned, batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwAckY62z7s9"
   },
   "source": [
    "### a- Training\n",
    "\n",
    "Since the phonemes are not aligned, there is no simple direct way to get the classification acuracy of a model. Write and test the three functions ```train_one_epoch_ctc```, ```validation_step_ctc``` and ```run_ctc``` as before but without considering the average acuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "oYg5YzW8EHl4",
    "outputId": "be8aec47-f0a8-4cb6-b4ff-df232118a6d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint_data/checkpoint_30.pt\n",
      "Loading the state dict at checkpoint_data/checkpoint_30.pt\n"
     ]
    }
   ],
   "source": [
    "from cpc.feature_loader import loadModel\n",
    "\n",
    "checkpoint_path = 'checkpoint_data/checkpoint_30.pt'\n",
    "cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n",
    "cpc_model = cpc_model.cuda()\n",
    "phone_classifier = PhoneClassifier(HIDDEN_CONTEXT_MODEL, N_PHONES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CFQ2g3PjErdZ"
   },
   "outputs": [],
   "source": [
    "parameters = list(phone_classifier.parameters()) + list(cpc_model.parameters())\n",
    "LEARNING_RATE = 2e-4\n",
    "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)\n",
    "\n",
    "optimizer_frozen = torch.optim.Adam(list(phone_classifier.parameters()), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zsgjv3cD0oqD"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train_one_epoch_ctc(cpc_model, \n",
    "                        phone_classifier, \n",
    "                        loss_criterion, \n",
    "                        data_loader, \n",
    "                        optimizer):\n",
    "    \n",
    "    cpc_model.train()\n",
    "    loss_criterion.train()\n",
    "    \n",
    "    avg_loss = 0\n",
    "    avg_accuracy = 0\n",
    "    n_items = 0\n",
    "    for step, full_data in enumerate(data_loader):\n",
    "\n",
    "        x, x_len, y, y_len = full_data\n",
    "\n",
    "        x_batch_len = x.shape[-1]\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        bs=x.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        context_out, enc_out, _ = cpc_model(x.to(device),y.to(device))\n",
    "\n",
    "        scores = phone_classifier(context_out)\n",
    "        scores = scores.permute(1,0,2)\n",
    "        scores = F.log_softmax(scores,2)\n",
    "        yhat_len = torch.tensor([int(scores.shape[0]*x_len[i]/x_batch_len) for i in range(scores.shape[1])]) # this is an approximation, should be good enough\n",
    "\n",
    "        loss = loss_criterion(scores,y.to(device),yhat_len,y_len)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss+=loss.item()*bs\n",
    "        n_items+=bs\n",
    "        \n",
    "    avg_loss/=n_items\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "def validation_step(cpc_model, \n",
    "                    phone_classifier, \n",
    "                    loss_criterion, \n",
    "                    data_loader):\n",
    "    \n",
    "    cpc_model.eval()\n",
    "    phone_classifier.eval()\n",
    "    avg_loss = 0\n",
    "    avg_accuracy = 0\n",
    "    n_items = 0\n",
    "    with torch.no_grad():\n",
    "        for step, full_data in enumerate(data_loader):\n",
    "            \n",
    "            x, x_len, y, y_len = full_data\n",
    "            x_batch_len = x.shape[-1]\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            bs=x.size(0)\n",
    "            context_out, enc_out, _ = cpc_model(x.to(device),y.to(device))\n",
    "            \n",
    "            scores = phone_classifier(context_out)\n",
    "            scores = scores.permute(1,0,2)\n",
    "            scores = F.log_softmax(scores,2)\n",
    "            yhat_len = torch.tensor([int(scores.shape[0]*x_len[i]/x_batch_len) for i in range(scores.shape[1])]) # this is an approximation, should be good enough\n",
    "            \n",
    "            loss = loss_criterion(scores,y.to(device),yhat_len,y_len)\n",
    "            avg_loss+=loss.item()*bs\n",
    "            n_items+=bs\n",
    "            \n",
    "    avg_loss/=n_items\n",
    "    return avg_loss\n",
    "\n",
    "def run_ctc(cpc_model, \n",
    "            phone_classifier, \n",
    "            loss_criterion, \n",
    "            data_loader_train, \n",
    "            data_loader_val, \n",
    "            optimizer,\n",
    "            n_epoch):\n",
    "    for epoch in range(n_epoch):\n",
    "\n",
    "        print(f\"Running epoch {epoch + 1} / {n_epoch}\")\n",
    "        loss_train = train_one_epoch_ctc(cpc_model, phone_classifier, loss_criterion, data_loader_train, optimizer)\n",
    "        print(\"-------------------\")\n",
    "        print(f\"Training dataset :\")\n",
    "        print(f\"Average loss : {loss_train}.\")\n",
    "\n",
    "        print(\"-------------------\")\n",
    "        print(\"Validation dataset\")\n",
    "        loss_val = validation_step(cpc_model, phone_classifier, loss_criterion, data_loader_val)\n",
    "        print(f\"Average loss : {loss_val}\")\n",
    "        print(\"-------------------\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GSr7tcUdD72c",
    "outputId": "ea9f1f53-6b42-436d-e760-68bfdae72bf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 40.1225111990264.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 36.384601551402696\n",
      "-------------------\n",
      "\n",
      "Running epoch 2 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 34.704571810635656.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 31.07986525102095\n",
      "-------------------\n",
      "\n",
      "Running epoch 3 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 29.263464641308325.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 25.670075073242188\n",
      "-------------------\n",
      "\n",
      "Running epoch 4 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 23.94062240852797.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 20.725055042613636\n",
      "-------------------\n",
      "\n",
      "Running epoch 5 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 19.358014251246598.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 16.735400938554243\n",
      "-------------------\n",
      "\n",
      "Running epoch 6 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 15.754520962717777.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 13.685918433449485\n",
      "-------------------\n",
      "\n",
      "Running epoch 7 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 13.06561186031205.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 11.475975223888051\n",
      "-------------------\n",
      "\n",
      "Running epoch 8 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 11.12484725710446.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 9.905263519287109\n",
      "-------------------\n",
      "\n",
      "Running epoch 9 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 9.739885385371437.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 8.803418502807617\n",
      "-------------------\n",
      "\n",
      "Running epoch 10 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 8.756830835473767.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 8.009756735021417\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_ctc(cpc_model,phone_classifier,loss_ctc,data_loader_train,data_loader_val,optimizer_frozen,n_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TKrYW4gK1BBF"
   },
   "source": [
    "### b- Evaluation: the Phone Error Rate (PER)\n",
    "\n",
    "In order to compute the similarity between two sequences, we can use the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance). This distance estimates the minimum number of insertion, deletion and addition to move from one sequence to another. If we normalize this distance by the number of characters in the reference sequence we get the Phone Error Rate (PER).\n",
    "\n",
    "This value can be interpreted as :\n",
    "\\\\[  PER = \\frac{S + D + I}{N} \\\\]\n",
    "\n",
    "Where:\n",
    "\n",
    "\n",
    "*   N is the number of characters in the reference\n",
    "*   S is the number of substitutiion\n",
    "*   I in the number of insertion\n",
    "*   D in the number of deletion\n",
    "\n",
    "For the best possible alignment of the two sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RoBhsx7GNqI_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_PER_sequence(ref_seq, target_seq):\n",
    "    \n",
    "    # re = g.split()\n",
    "    # h = h.split()\n",
    "    n = len(ref_seq)\n",
    "    m = len(target_seq)\n",
    "    \n",
    "    D = np.zeros((n+1,m+1))\n",
    "    for i in range(1,n+1):\n",
    "        D[i,0] = D[i-1,0]+1\n",
    "    for j in range(1,m+1):\n",
    "        D[0,j] = D[0,j-1]+1\n",
    "        \n",
    "    ### TODO compute the alignment\n",
    "    for i in range(1,n+1):\n",
    "        for j in range(1,m+1):\n",
    "            D[i,j] = min(\n",
    "                D[i-1,j]+1,\n",
    "                D[i-1,j-1]+1,\n",
    "                D[i,j-1]+1,\n",
    "                D[i-1,j-1]+ 0 if ref_seq[i-1]==target_seq[j-1] else float(\"inf\")\n",
    "            )\n",
    "            \n",
    "    return D[n,m]/len(ref_seq)\n",
    "\n",
    "    #return PER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r-hr0KK0mgcR"
   },
   "source": [
    "You can test your function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AfTb3yOQmvey",
    "outputId": "5db807ca-4c58-4413-ab19-6581d7d07263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "ref_seq = [0, 1, 1, 2, 0, 2, 2]\n",
    "pred_seq = [1, 1, 2, 2, 0, 0]\n",
    "\n",
    "expected_PER = 4. / 7.\n",
    "print(get_PER_sequence(ref_seq, pred_seq) == expected_PER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nHiyChl-m_k7"
   },
   "source": [
    "## c- Evaluating the PER of your model on the test dataset\n",
    "\n",
    "Evaluate the PER on the validation dataset. Please notice that you should usually use a separate dataset, called the dev dataset, to perform this operation. However for the sake of simplicity we will work with validation data in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DMkX0PoFnclg"
   },
   "outputs": [],
   "source": [
    "import progressbar\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def cut_data(seq, sizeSeq):\n",
    "    maxSeq = sizeSeq.max()\n",
    "    return seq[:, :maxSeq]\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    seq, sizeSeq, phone, sizePhone = data\n",
    "    seq = seq.cuda()\n",
    "    phone = phone.cuda()\n",
    "    sizeSeq = sizeSeq.cuda().view(-1)\n",
    "    sizePhone = sizePhone.cuda().view(-1)\n",
    "\n",
    "    seq = cut_data(seq.permute(0, 2, 1), sizeSeq).permute(0, 2, 1)\n",
    "    return seq, sizeSeq, phone, sizePhone\n",
    "\n",
    "\n",
    "def get_per(test_dataloader,\n",
    "            cpc_model,\n",
    "            phone_classifier):\n",
    "    \n",
    "    downsampling_factor = 160\n",
    "    cpc_model.eval()\n",
    "    phone_classifier.eval()\n",
    "    \n",
    "    avgPER = 0\n",
    "    nItems = 0 \n",
    "    \n",
    "    print(\"Starting the PER computation through beam search\")\n",
    "    bar = progressbar.ProgressBar(maxval=len(test_dataloader))\n",
    "    bar.start()\n",
    "    \n",
    "    for index, data in enumerate(test_dataloader):\n",
    "\n",
    "        bar.update(index)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            seq, sizeSeq, phone, sizePhone = prepare_data(data)\n",
    "            c_feature, _, _ = cpc_model(seq.to(device),phone.to(device))\n",
    "            sizeSeq = sizeSeq / downsampling_factor\n",
    "            predictions = torch.nn.functional.softmax(\n",
    "            phone_classifier(c_feature), dim=2).cpu()\n",
    "            phone = phone.cpu()\n",
    "            sizeSeq = sizeSeq.cpu()\n",
    "            sizePhone = sizePhone.cpu()\n",
    "\n",
    "            bs = c_feature.size(0)\n",
    "            data_per = [(predictions[b].argmax(1),  phone[b]) for b in range(bs)]\n",
    "            # data_per = [(predictions[b], sizeSeq[b], phone[b], sizePhone[b],\n",
    "            #               \"criterion.module.BLANK_LABEL\") for b in range(bs)]\n",
    "\n",
    "            with Pool(bs) as p:\n",
    "                poolData = p.starmap(get_PER_sequence, data_per)\n",
    "            avgPER += sum([x for x in poolData])\n",
    "            nItems += len(poolData)\n",
    "            \n",
    "    bar.finish()\n",
    "    \n",
    "    avgPER /= nItems\n",
    "\n",
    "    print(f\"Average PER {avgPER}\")\n",
    "    \n",
    "    return avgPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "2hvnudh4Osb4",
    "outputId": "acdd8ab7-c766-4a2b-9b6c-b744266d6240"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% |                                                                        |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the PER computation through beam search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PER 0.9478332927447669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9478332927447669"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_per(data_loader_val,cpc_model,phone_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p8e9D7g8159k"
   },
   "source": [
    "## Exercice 3 : Character error rate (CER) \n",
    "\n",
    "**The Character Error Rate (CER) is an evaluation metric similar to the PER but with characters insterad of phonemes.** Using the following data, run the functions you defined previously to estimate the CER of your model after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "cXONmKQOuFSn",
    "outputId": "5a2791ec-ff0e-4749-ce93-92922ffbee01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:00, 6353.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cache file at ../MaseseSpeech/train-clean/_seqs_cache.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "12it [00:00, 4387.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 364 sequences in 2.56 seconds\n",
      "maxSizeSeq : 406560\n",
      "maxSizePhone : 199\n",
      "minSizePhone : 17\n",
      "Total size dataset 1.28108578125 hours\n",
      "Saved cache file at ../MaseseSpeech/dev-clean/_seqs_cache.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 276 sequences in 2.46 seconds\n",
      "maxSizeSeq : 356829\n",
      "maxSizePhone : 175\n",
      "minSizePhone : 57\n",
      "Total size dataset 0.9148587326388888 hours\n"
     ]
    }
   ],
   "source": [
    "# Load a dataset labelled with the letters of each sequence.\n",
    "# %cd /content/CPC_audio\n",
    "from cpc.eval.common_voices_eval import SingleSequenceDataset, parseSeqLabels, findAllSeqs\n",
    "path_train_data_cer = '../MaseseSpeech/train-clean/'\n",
    "path_val_data_cer = '../MaseseSpeech/dev-clean'\n",
    "path_letter_data_cer = '../MaseseSpeech/converted_aligned_phones.txt'\n",
    "\n",
    "BATCH_SIZE=8\n",
    "\n",
    "letters_labels, N_LETTERS = parseSeqLabels(path_letter_data_cer)\n",
    "data_train_cer, _ = findAllSeqs(path_train_data_cer, extension='.wav')\n",
    "dataset_train_non_aligned = SingleSequenceDataset(path_train_data_cer, data_train_cer, letters_labels)\n",
    "\n",
    "\n",
    "data_val_cer, _ = findAllSeqs(path_val_data_cer, extension='.wav')\n",
    "dataset_val_non_aligned = SingleSequenceDataset(path_val_data_cer, data_val_cer, letters_labels)\n",
    "\n",
    "\n",
    "# The data loader will generate a tuple of tensors data, labels for each batch\n",
    "# data : size N x T1 x 1 : the audio sequence\n",
    "# label : size N x T2 the sequence of letters corresponding to the audio data\n",
    "# IMPORTANT NOTE: just like the PER the CER is computed with non-aligned phone data.\n",
    "data_loader_train_letters = torch.utils.data.DataLoader(dataset_train_non_aligned, batch_size=BATCH_SIZE,\n",
    "                                                shuffle=True)\n",
    "data_loader_val_letters = torch.utils.data.DataLoader(dataset_val_non_aligned, batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "9h07zI2LjzAU",
    "outputId": "70542ed5-a32e-448f-83ea-6355e601511e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint_data/checkpoint_30.pt\n",
      "Loading the state dict at checkpoint_data/checkpoint_30.pt\n"
     ]
    }
   ],
   "source": [
    "from cpc.feature_loader import loadModel\n",
    "\n",
    "checkpoint_path = 'checkpoint_data/checkpoint_30.pt'\n",
    "cpc_model, HIDDEN_CONTEXT_MODEL, HIDDEN_ENCODER_MODEL = loadModel([checkpoint_path])\n",
    "cpc_model = cpc_model.cuda()\n",
    "character_classifier = PhoneClassifier(HIDDEN_CONTEXT_MODEL, N_LETTERS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHCNg1E7lW1L"
   },
   "outputs": [],
   "source": [
    "parameters = list(character_classifier.parameters()) + list(cpc_model.parameters())\n",
    "LEARNING_RATE = 2e-4\n",
    "optimizer = torch.optim.Adam(parameters, lr=LEARNING_RATE)\n",
    "\n",
    "optimizer_frozen = torch.optim.Adam(list(character_classifier.parameters()), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "engpkljbk9hj"
   },
   "outputs": [],
   "source": [
    "loss_ctc = torch.nn.CTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9NBHd2s2kxld",
    "outputId": "24067818-b4b2-4f90-a433-a63f851b7587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 38.6691555096755.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 34.953639124090024\n",
      "-------------------\n",
      "\n",
      "Running epoch 2 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 33.216111889883834.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 29.625256576538085\n",
      "-------------------\n",
      "\n",
      "Running epoch 3 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 27.826208592774783.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 24.4285249189897\n",
      "-------------------\n",
      "\n",
      "Running epoch 4 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 22.75667438034184.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 19.74463417746804\n",
      "-------------------\n",
      "\n",
      "Running epoch 5 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 18.403470472856.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 16.012580705122513\n",
      "-------------------\n",
      "\n",
      "Running epoch 6 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 15.040938335345139.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 13.188155201998624\n",
      "-------------------\n",
      "\n",
      "Running epoch 7 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 12.560057374728284.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 11.209445190429687\n",
      "-------------------\n",
      "\n",
      "Running epoch 8 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 10.807398835489572.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 9.816126504377886\n",
      "-------------------\n",
      "\n",
      "Running epoch 9 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 9.56781629031683.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 8.832395907315341\n",
      "-------------------\n",
      "\n",
      "Running epoch 10 / 10\n",
      "-------------------\n",
      "Training dataset :\n",
      "Average loss : 8.689204124051349.\n",
      "-------------------\n",
      "Validation dataset\n",
      "Average loss : 8.132575406161221\n",
      "-------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_ctc(cpc_model,character_classifier,loss_ctc,data_loader_train_letters,data_loader_val_letters,optimizer_frozen,n_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "A8oxFr1jm17P",
    "outputId": "947ea6da-7fd9-4106-e3f1-32cf6d77c4f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% |                                                                        |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the PER computation through beam search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PER 0.9495332706026778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9495332706026778"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_per(data_loader_val_letters,cpc_model,character_classifier)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}